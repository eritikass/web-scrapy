import crawlbase    # base crawl
import re
import scrapy


class ExploitDbSpider(crawlbase.BaseSpider):
    baseurl = 'https://www.exploit-db.com'
    start_urls = [baseurl + '/platform/?order_by=date&order=desc&pg=240&p=php']
    name = 'exploit-db'

    def parse(self, response):
        self.parse_page(response)

        for page in response.css('.pagination a'):  #:attr("href")').re('.*/platform/.*'):
            txt = page.css('::text').extract_first()
            if txt in "next":
                url = page.css('::attr("href")').extract_first()
                print url
                yield scrapy.Request(response.urljoin(url), self.parse)
                break

    def parse_page(self, response):
        for post in response.css('td.description a'):
            title = post.css('::text').extract_first()

            if self.is_wp(title):
                url = post.css('::attr("href")').extract_first()

                provider_id = self.get_provider_id(url)
                #print provider_id

                if self.used(provider_id):
                    #print "used"
                    continue

                ep = title.split(' - ')
                exploit = ep.pop()
                mod2 = self.cleanup_str(" ".join(ep))

                vp = mod2.split(' ')
                version = vp.pop()

                if version and version.replace('<', '').replace('=', '').replace('.', '').replace(' ', '').isnumeric():
                    mod2 = " ".join(vp)
                else:
                    exploit = self.cleanup_str(exploit)
                    mod2 = exploit
                    version = ''

                print "add: " + title + " < " + url

                if self.baseurl not in url:
                    url = self.baseurl + url

                self.insert(provider_id=provider_id, link=url, module=mod2, exploit=exploit, version=version)
